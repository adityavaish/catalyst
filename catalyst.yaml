# ===========================================================================
#  Catalyst Configuration
#
#  This is the main config file.  Environment variables can be referenced
#  with ${ENV_VAR} syntax.
# ===========================================================================

app_name: "Catalyst"
host: "0.0.0.0"
port: 8000
debug: true
log_level: "INFO"

# Directory containing prompt YAML files (scanned recursively)
prompts_dir: "prompts"

# Cross-Origin Resource Sharing
cors_origins:
  - "*"

# ---------------------------------------------------------------------------
# Response Caching
# ---------------------------------------------------------------------------
cache:
  enabled: true
  max_size: 1000           # max cached responses in memory
  default_ttl: 300         # default TTL in seconds (per-endpoint overrides via cache_ttl)

# ---------------------------------------------------------------------------
# Performance Tuning
# ---------------------------------------------------------------------------
performance:
  parallel_tool_calls: true          # run multiple tool calls concurrently
  circuit_breaker_enabled: true      # fail fast when LLM is degraded
  circuit_breaker_threshold: 5       # consecutive failures before opening
  circuit_breaker_recovery: 30.0     # seconds before probing recovery
  llm_timeout: 60.0                  # per-call timeout in seconds
  execution_plans:
    enabled: true                    # cache LLM-generated execution plans
    plan_ttl: 3600                   # seconds before a plan expires (0 = infinite)
    max_plans: 500                   # max cached plans in memory
    max_errors: 3                    # consecutive plan errors before invalidation
    background_refresh: true         # proactively regenerate plans before expiry

# ---------------------------------------------------------------------------
# LLM Configuration
# ---------------------------------------------------------------------------
llm:
  provider: "azure_openai"   # openai | anthropic | azure_openai | ollama | litellm
  model: "azure/gpt-4o-mini"  # Must be prefixed with azure/ for Azure OpenAI
  api_key: "${AZURE_OPENAI_API_KEY}"
  api_base: "${AZURE_OPENAI_ENDPOINT}"  # e.g. https://your-resource.openai.azure.com
  api_version: "2024-12-01-preview"
  azure_deployment: "gpt-4o-mini"
  temperature: 0.0
  max_tokens: 4096

# ---------------------------------------------------------------------------
# Connectors â€” databases, MCP servers, HTTP services
# ---------------------------------------------------------------------------
connectors:
  - name: math_eval
    type: math

# Example connectors (uncomment as needed):
#
#   # -- PostgreSQL --
#   - name: main_db
#     type: postgres
#     connection_string: "${DATABASE_URL}"
#
#   # -- SQLite (great for local dev) --
#   - name: main_db
#     type: sqlite
#     connection_string: "sqlite:///./data/catalyst.db"
#
#   # -- MongoDB --
#   - name: mongo
#     type: mongodb
#     connection_string: "${MONGODB_URI}"
#     options:
#       database: "myapp"
#
#   # -- Redis --
#   - name: cache
#     type: redis
#     connection_string: "redis://localhost:6379"
#
#   # -- External HTTP API --
#   - name: weather_api
#     type: http
#     connection_string: "https://api.openweathermap.org"
#     options:
#       headers:
#         x-api-key: "${WEATHER_API_KEY}"
#       timeout: 10
#
#   # -- MCP Server (stdio) --
#   - name: filesystem
#     type: mcp
#     mcp_server_command: "npx"
#     mcp_server_args: ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
#
#   # -- MCP Server (SSE) --
#   - name: remote_tools
#     type: mcp
#     mcp_server_url: "http://localhost:3001/sse"
#
#   # -- Elasticsearch --
#   - name: search
#     type: elasticsearch
#     connection_string: "http://localhost:9200"
